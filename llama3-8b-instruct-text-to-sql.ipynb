{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f7396-ac88-44e5-85af-e0b286e457ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc2ece-ea09-4788-97e7-70c8619be4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  --upgrade \"transformers\"   \"datasets\"  \"accelerate\"  \"evaluate\"  \"bitsandbytes\"  \"trl\"  \"peft\" \n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635e68d-63b1-4a4c-a378-767d634314e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install ninja packaging\n",
    "    !pip install flash-attn --no-build-isolation\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2a357-295e-4e2d-993f-2d87c1821b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"\", # ADD YOUR TOKEN HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f8ef4-6d7a-4594-b77d-29d9b60d9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a387de-a9d3-4e2d-9dfd-0cf4469eaf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557f6f9-ad92-4fdf-a93e-af2c9c4ac716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "# {\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "\n",
    "# \"\"\"\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    \n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6c12-3d13-486a-b6b8-760c93a1b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7b780-132c-4dd9-9548-4ee70cceb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a199d37-b8a8-4e09-ae62-ce35d5e824a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a39bdc-79c8-434e-a5fd-19040e67c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824dbfd-8dd6-4585-bb15-8d4a80df65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef869d7-4097-4c9d-ac63-a2aa38820150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d848c1-dda7-4c63-9232-8a53a7f479a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd955d-357e-4b4d-bf9c-b8cffbdf4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951d7b4-f5d1-4cc9-a456-24bd4afc5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ff4dc-c416-418f-bfbc-2d2818b15ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebc3d5-cee9-4547-a4ad-84c285b0399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama3-8b-instruct-text-to-sql\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef5608-343b-4d67-90e1-eeec78f52418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164a303-844c-4a47-bd72-e2fe5f51e6ff",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daec8c-914d-4962-9d1a-b07160c2d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbc1d3-3a21-470b-8700-95eda6de4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5693e8-e0e9-48ac-bbf3-b0245eb0091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf814cae-105c-4078-a5a4-9a68b6b3f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"YOUR_HUGGINEFACE_USERNAME/llama3-8b-instruct-text-to-sql\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257e560-a586-4604-b2f8-cc90bb919641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE match_season (College VARCHAR, POSITION VARCHAR)\"},\n",
    "    {\"role\": \"user\", \"content\": \"Which college have both players with position midfielder and players with position defender?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a1b60-9c52-4f0f-8021-883227009a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "\n",
    "\n",
    "def evaluate(sample):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        sample[\"messages\"][:2],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    predicted_answer=tokenizer.decode(response, skip_special_tokens=True).strip()\n",
    "    truth = sample[\"messages\"][2][\"content\"]\n",
    "    print(f\"predict {predicted_answer}\")\n",
    "    print(f\"truth {truth}\")\n",
    "    print(\"*******************\")\n",
    "    \n",
    "    if predicted_answer == truth:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "success_rate = []\n",
    "number_of_eval_samples = 1000\n",
    "# iterate over eval dataset and predict\n",
    "for s in tqdm(dataset.shuffle().select(range(number_of_eval_samples))):\n",
    "    success_rate.append(evaluate(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99721c60-a942-416d-9757-8c5113f70034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
